{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the basic toml config file, and make iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import os\n",
    "# from transformers import AutoModel, BertModel, AutoModelForMaskedLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir :\t data\n",
      "train_data :\t data/english_train_head.jsonlines\n",
      "dev_data :\t data/english_development_head.jsonlines\n",
      "test_data :\t data/english_test_head.jsonlines\n",
      "device :\t cuda:0\n",
      "bert_model :\t bert-large-cased\n",
      "bert_window_size :\t 512\n",
      "embedding_size :\t 20\n",
      "sp_embedding_size :\t 64\n",
      "a_scoring_batch_size :\t 512\n",
      "hidden_size :\t 1024\n",
      "n_hidden_layers :\t 1\n",
      "max_span_len :\t 64\n",
      "rough_k :\t 50\n",
      "bert_finetune :\t True\n",
      "dropout_rate :\t 0.3\n",
      "bert_learning_rate :\t 1e-05\n",
      "learning_rate :\t 0.0003\n",
      "train_epochs :\t 20\n",
      "bce_loss_weight :\t 0.5\n",
      "conll_log_dir :\t data/conll_logs\n",
      "tokenizer_kwargs :\t {'roberta-large': {'add_prefix_space': True}, 'spanbert-large-cased': {'do_lower_case': False}, 'bert-large-cased': {'do_lower_case': False}}\n"
     ]
    }
   ],
   "source": [
    "source = \"/home/egil/gits_wsl/wl-coref-ncc/config.toml\"\n",
    "toml_folder = \"/home/egil/gits_wsl/wl-coref-ncc/tomls\"\n",
    "train_path = \"/home/egil/gits_wsl/ncc/wl-ncc_heads/wl-ncc_train_head.jsonl\"\n",
    "runpath = \"/home/egil/gits_wsl/wl-coref-ncc/run.py\"\n",
    "\n",
    "with open(source) as rf:\n",
    "    base_toml ={'DEFAULT': toml.loads(rf.read())['DEFAULT']}\n",
    "\n",
    "defaults = base_toml['DEFAULT']\n",
    "for key, value in defaults.items():\n",
    "    print(key,\":\\t\", value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes from the default that is shared by all experiments go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"models01\"\n",
    "\n",
    "defaults[\"device\"] = \"cpu\"\n",
    "defaults[\"bert_finetune\"] = False\n",
    "defaults[\"train_epochs\"] = 2\n",
    "defaults[\"train_data\"] = train_path\n",
    "defaults[\"dev_data\"] = train_path.replace(\"train\", \"development\")\n",
    "defaults[\"test_data\"] = train_path.replace(\"train\", \"test\")\n",
    "out_folder = os.path.join(toml_folder, run_id)\n",
    "defaults[\"conll_log_dir\"] = os.path.join(out_folder, \"conll_logs\")\n",
    "defaults[\"data_dir\"] = out_folder\n",
    "if not os.path.exists(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "    if not os.path.exists(defaults[\"conll_log_dir\"]):\n",
    "        os.mkdir(defaults[\"conll_log_dir\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lists of what should be iterated over, and write a toml file with each of these experiments in the one file\n",
    "Start without any grid, list only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternatives = {\"bert_models\": [\"xlm-roberta-base\", \"bert-base-multilingual-cased\",\"/home/egil/datasets/norbert2\", \"/home/egil/datasets/nb-bert-base\",]}\n",
    "exp_ids = []\n",
    "out_toml = {'DEFAULT': defaults}\n",
    "for key, alts in alternatives.items():\n",
    "    param_name = key[:-1] # Always add s also when s from before\n",
    "    for idx, alt in enumerate(alts):\n",
    "        experiment_id = run_id+\"_\"+str(idx).zfill(3)\n",
    "        out_toml[experiment_id] = {param_name: alt}\n",
    "        exp_ids.append(experiment_id)\n",
    "out_toml_path = os.path.join(toml_folder, run_id+\".toml\")\n",
    "with open(out_toml_path, \"w\") as wf:\n",
    "    toml.dump(out_toml, wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norwegian models\n",
    "Got them to run with cloned and local path, and torch=1.6 in stead of 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we can find the models:\n",
    "\n",
    "# am_lms = [\"ltgoslo/norbert2\", \"NbAiLab/nb-bert-base\", \"flax-community/roberta-base-scandinavian\" ]\n",
    "for model in []: # alternatives[\"bert_models\"]:\n",
    "    try:\n",
    "        m = AutoModel.from_pretrained(model).to(\"cpu\")\n",
    "        print(\"success\", model)\n",
    "    except OSError as e:\n",
    "        print(\"failure\", model)\n",
    "        # print(e, \"\\n\")\n",
    "\n",
    "# model = \"/home/egil/datasets/norbert2\"\n",
    "# model = \"/home/egil/datasets/nb-bert-base\"\n",
    "# m = AutoModel.from_pretrained(model, local_files_only=True).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/egil/gits_wsl/wl-coref-ncc/tomls/models01.sh\n",
      "/home/egil/gits_wsl/wl-coref-ncc/run.py\n",
      "#!/bin/sh\n",
      "python /home/egil/gits_wsl/wl-coref-ncc/run.py train models01_000 --config-file /home/egil/gits_wsl/wl-coref-ncc/tomls/models01.toml\n",
      "python /home/egil/gits_wsl/wl-coref-ncc/run.py train models01_001 --config-file /home/egil/gits_wsl/wl-coref-ncc/tomls/models01.toml\n",
      "python /home/egil/gits_wsl/wl-coref-ncc/run.py train models01_002 --config-file /home/egil/gits_wsl/wl-coref-ncc/tomls/models01.toml\n",
      "python /home/egil/gits_wsl/wl-coref-ncc/run.py train models01_003 --config-file /home/egil/gits_wsl/wl-coref-ncc/tomls/models01.toml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "script_path = os.path.join(toml_folder, run_id+\".sh\")\n",
    "scriptlines  = [\"#!/bin/sh\"]\n",
    "for exp in exp_ids:\n",
    "    scriptlines.append(\" \".join([\"python\", runpath, \"train\", exp, \"--config-file\", out_toml_path]))\n",
    "\n",
    "with open (script_path, \"w\") as wf:\n",
    "    wf.write(\"\\n\".join(scriptlines))\n",
    "print(script_path)\n",
    "print(runpath)\n",
    "print(\"\\n\".join(scriptlines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install git-lfs\n",
    "# git lfs install\n",
    "# git clone https://huggingface.co/ltgoslo/norbert2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wl-coref')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb0bc5864f2c22f379ce753dde65f08012da028e1ec5eee31107dc3ec98f87c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
