[DEFAULT]
data_dir = "/fp/homes01/u01/ec-egilron/wl-coref-ncc/tomls/models03"
train_data = "/fp/homes01/u01/ec-egilron/ncc/wl-ncc_heads/wl-ncc_train_head.jsonl"
dev_data = "/fp/homes01/u01/ec-egilron/ncc/wl-ncc_heads/wl-ncc_development_head.jsonl"
test_data = "/fp/homes01/u01/ec-egilron/ncc/wl-ncc_heads/wl-ncc_test_head.jsonl"
device = "cuda:0"
bert_model = "/fp/homes01/u01/ec-egilron/transformers/nb-bert-base"
bert_window_size = 512
embedding_size = 20
sp_embedding_size = 64
a_scoring_batch_size = 512
hidden_size = 1024
n_hidden_layers = 1
max_span_len = 64
rough_k = 50
bert_finetune = true
dropout_rate = 0.3
bert_learning_rate = 1e-5
learning_rate = 0.0003
train_epochs = 4
bce_loss_weight = 0.5
conll_log_dir = "/fp/homes01/u01/ec-egilron/wl-coref-ncc/tomls/models03/conll_logs"

[models03_000]
a_scoring_batch_size = 1

[models03_001]
a_scoring_batch_size = 2

[models03_002]
a_scoring_batch_size = 4

[models03_003]
a_scoring_batch_size = 8

[models03_004]
a_scoring_batch_size = 16

[models03_005]
a_scoring_batch_size = 32

[models03_006]
a_scoring_batch_size = 64

[models03_007]
a_scoring_batch_size = 128

[models03_008]
a_scoring_batch_size = 254

[DEFAULT.tokenizer_kwargs.roberta-large]
add_prefix_space = true

[DEFAULT.tokenizer_kwargs.spanbert-large-cased]
do_lower_case = false

[DEFAULT.tokenizer_kwargs.bert-large-cased]
do_lower_case = false
